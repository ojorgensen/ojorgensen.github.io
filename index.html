---
layout: home
title: Ole Jorgensen
---

<div class="home">
  <h2> <b> Overview of me: </b> </h2>
  
  <figure>
  <img src="/img/OleFace.jpg" alt="OleFace" style="width:200px;height:200px;"/>
    <figcaption><i> This is me!</i> </figcaption>
</figure>

<p> Hello! My name is Ole, and I am a research engineer currently working at the AI Safety Institute with the Chem-Bio team. I've previously led research on language model evaluations and interpretability. 
  I previously completed an MSc in Artificial Intelligence from Imperial College London, and have received an MMath in Mathematics from the University of St Andrews.</p>

<p>
  I am always keen to talk to folks about AI safety, evals, and interpretability. I'm thinking about mentoring junior students on evaluations projects. If you might be interested, please reach out!
</p>

  
  <h2> <b> Recent Work:</b> </h2>
  <p><a href="https://www.aisi.gov.uk/work/early-insights-from-developing-question-answer-evaluations-for-frontier-ai">Early Insights from Developing Question-Answer Evaluations for Frontier AI</a> I wrote this blog post alongside Friederike Grosse-Holz! We share lots of in the weeds details we have learned from developing and conducting QA evaluations.</p>

  <p><a href="https://www.aisi.gov.uk/work/advanced-ai-evaluations-may-update">Advanced AI evaluations at AISI: May update</a> - I am proud to have contributed to this work! I helped design and implement our methodology for evaluating the chemical and biological capabilities of frontier language models.</p>

  <p><a href="https://arxiv.org/abs/2312.03813">"Improving Activation Steering in Language Models with Mean-Centring"</a>, <b>winner of the best paper award</b> at Human-Centric Representation Learning at AAAI 2024. Ole Jorgensen, Dylan Cope, Nandi Schoots, Murray Shanahan. 
    A follow up to work from my Dissertation, we develop a new method of activation steering, called mean-centring, which is more general than previous methods. We evaluate it in a variety of settings, including applying it to recent work on <a href="https://functions.baulab.info/">function vectors by Eric Todd et. al.</a></p>
  
  <p><a href="https://arxiv.org/abs/2310.13439">"Self-Consistency of Large Language Models under Ambiguity"</a>, BlackboxNLP Workshop at EMNLP 2023. Henning Bartsch, Ole Jorgensen, Domenic Rosati, Jason Hoelscher-Obermaier, Jacob Pfau.
  Completed as part of AI Safety Camp. We investigated the self-consistency of various OpenAI models under ambiguity, using the novel setting of integer sequences.</p>

  <p>I completed my Dissertation on investigating latent spaces of transformer models. My supervisors were <a href="https://www.doc.ic.ac.uk/~mpsha/">Murray Shanahan</a>, <a href="https://dylancope.com/">Dylan Cope</a>, and <a href="https://nandischoots.com/">Nandi Schoots</a>, who I have all enjoyed working with immensely.
    Specifically we investigated when transformer models represent features geometrically, and how this can be used to better control models (such as via <a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector">activation additions</a> and feature detection).
    You can see my Dissertation <a href="assets/pdfs/Imperial_Dissertation.pdf">here</a>.
  </p>

  

</div>
