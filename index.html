---
layout: home
title: Ole Jorgensen
---

<div class="home">
  <h2> <b> Overview of me: </b> </h2>
  
  <figure>
  <img src="/img/OleFace.jpg" alt="OleFace" style="width:200px;height:200px;"/>
    <figcaption><i> This is me!</i> </figcaption>
</figure>

<p> Hello! My name is Ole, and I am an AI MSc Student at Imperial College London. Before this, I graduated with a MMath degree in Mathematics from the University of St Andrews.</p>

  <h2> <b> What I'm Working on: </b> </h2>
  <p>I'm finishing work on my Dissertation, which has focussed on investigating latent spaces of transformer models. My supervisors are Murray Shanahan, Dylan Cope, and Nandi Schoots, who I have all enjoyed working with immensely.
    Specifically, I've been looking at when transformer models represent features geometrically, and how this can be used to better control models (such as via <a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector">activation additions</a> and feature detection).
    For a little update on my progress, I have written two quick blog posts: one on the impact of LayerNorm on how features will be represented in transformers; and another thinking about why counterbalanced subtractions seem to be necessary for activation steering. You can find them <a href="https://www.lesswrong.com/posts/pvCpvMjjyXaAXtnSi/because-of-layernorm-directions-in-gpt-2-mlp-layers-are">here</a> and <a href="https://www.lesswrong.com/posts/SgpMo2YMBJxwJaik9/understanding-counterbalanced-subtractions-for-better">here</a></p>

  <p>I'm also finishing up a paper on evaluating the introspective truthfulness of language models with the rest of my group from AI Safety Camp - more details on this after submission!</p>

  <p> Ever wondered how a humble 1-layer transformer finds the minimum of two numbers? Wonder no more: I investigated how in excrutiating detail, and the results were pretty interesting and surprisingly clean!</p>
  <p> I compiled these results into a Colab notebook, which you can find <a href="https://colab.research.google.com/drive/1iKx6Nz4iaRTdlyutDKSRhvebjtwaQE4L?usp=sharing">here</a>. If you have any thoughts on this, please feel free to email me, or comment on the Colab! </p> 

  
  <h2> <b> Thoughts on Alignment: </b> </h2>
  <p> Motivated by my interest in using my career to do the most good I can with my career, I am currently trying to better understand how we can create advanced AI systems which benefit humanity. I think of this as the field of AI Safety, and I think it might be the most important problem we will face this Century.</p>
  <p> I see a necessary (but not sufficient) aspect of answering this question as about solving alignment: creating systems which do what we want, even as they become more powerful. </p>
<p> I have been focussing pretty seriously on forming my own opinions about alignment, alongside skilling up, since my degree ended in June.</p>
<p> Some of this has resulted in a few posts about alignment, which I intend to continue writing over the next year or so. You can find this through either my <a href="https://ojorgensen.substack.com">substack</a>, or my <a href="https://www.lesswrong.com/users/ojorgensen">lesswrong!</a> </p>
 
  
  <h2><b> Other Things: </b> </h2>
  <p> Besides doing lots of Maths, the most valuable aspect of my degree was my involvement with the Effective Altruism Society. I was Co-President of the group for my last two years there.</p>
  <p> I get a huge amount of joy from playing sports / being outside, especially when with other people. I am super keen on football, despite being pretty bad at kicking things. I used to play for the St Andrews Futsal team, and I also ran the Shinty team at St Andrews for two years, which is like Scottish hockey with less rules (and more fun).</p>
  <p> I'm also super keen on music, and am periodically interested in literature, meditation, and poker. </p>

</div>
