---
layout: home
title: Ole Jorgensen
---

<div class="home">
  <h2> <b> Overview of me: </b> </h2>
  
  <figure>
  <img src="/img/OleFace.jpg" alt="OleFace" style="width:200px;height:200px;"/>
    <figcaption><i> This is me!</i> </figcaption>
</figure>

<p> Hello! My name is Ole, and I am an AI Safety researcher focussing on language model evaluations and interpretability. I am particularly interested in utilising an improved understanding of model activations to better control language models. 
  I recently completed an MSc in Artificial Intelligence from Imperial College London, and previously received an MMath in Mathematics from the University of St Andrews.
  I am involved with the AI Safety and Effective Altruism Communities.</p>

<p>
  I am currently applying for PhDs for 2024 start, and roles in ML engineering. I am also open to collaborating on research projects about language model interpretability - reach out if you might be interested!
</p>

  
  <h2> <b> Recent Work:</b> </h2>
  <p><a href="https://arxiv.org/abs/2312.03813">"Improving Activation Steering in Language Models with Mean-Centring"</a>, Human-Centric Representation Learning at AAAI 2024. Ole Jorgensen, Dylan Cope, Nandi Schoots, Murray Shanahan. 
    A follow up to work from my Dissertation, we develop a new method of activation steering, called mean-centring, which is more general than previous methods. We evaluate it in a variety of settings, including applying it to recent work on <a href="https://functions.baulab.info/">function vectors by Eric Todd et. al.</a></p>
  
  <p><a href="https://arxiv.org/abs/2310.13439">"Self-Consistency of Large Language Models under Ambiguity"</a>, BlackboxNLP Workshop at EMNLP 2023. Henning Bartsch, Ole Jorgensen, Domenic Rosati, Jason Hoelscher-Obermaier, Jacob Pfau.
  Completed as part of AI Safety Camp. We investigated the self-consistency of various OpenAI models under ambiguity, using the novel setting of integer sequences.</p>

  <p>I completed my Dissertation on investigating latent spaces of transformer models. My supervisors were <a href="https://www.doc.ic.ac.uk/~mpsha/">Murray Shanahan</a>, <a href="https://dylancope.com/">Dylan Cope</a>, and <a href="https://nandischoots.com/">Nandi Schoots</a>, who I have all enjoyed working with immensely.
    Specifically we investigated when transformer models represent features geometrically, and how this can be used to better control models (such as via <a href="https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector">activation additions</a> and feature detection).
    You can see my Dissertation <a href="assets/pdfs/Imperial_Dissertation.pdf">here</a>.
  </p>

  

</div>
